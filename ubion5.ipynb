{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ICh43289-9PseXHGeeJsG2g8yJi532BT",
      "authorship_tag": "ABX9TyPgCuBwKADyOR8Re91GwJ/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chajunhyeop/-/blob/main/ubion5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#신경망\n",
        "##분류예측"
      ],
      "metadata": {
        "id": "rALxdRgXa740"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Ashopping.csv\",encoding = 'cp949')"
      ],
      "metadata": {
        "id": "CJKxuW73bl38"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbTqzALMcA_U",
        "outputId": "da1c6dfd-f2d7-43bf-96a4-94bb6e6d8000"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-M3vcMZcCUI",
        "outputId": "8203a9d2-dad0-4b5e-9263-4d55653212d6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "고객ID         0\n",
              "이탈여부         0\n",
              "총매출액         0\n",
              "구매금액대        0\n",
              "방문빈도         0\n",
              "1회 평균매출액     0\n",
              "할인권 사용 횟수    0\n",
              "총 할인 금액      0\n",
              "고객등급         0\n",
              "구매유형         0\n",
              "클레임접수여부      0\n",
              "구매카테고리수      0\n",
              "거주지역         0\n",
              "성별           0\n",
              "고객 나이대       0\n",
              "거래기간         0\n",
              "할인민감여부       0\n",
              "Recency      0\n",
              "Frequency    0\n",
              "Monetary     0\n",
              "평균 구매주기      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "p0Zy8uwZcEXm",
        "outputId": "68312d06-cf99-4581-9794-6267a41c7e05"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              고객ID         이탈여부          총매출액        구매금액대        방문빈도  \\\n",
              "count  1000.000000  1000.000000  1.000000e+03  1000.000000  1000.00000   \n",
              "mean    500.500000     0.300000  5.858013e+06     0.700000    22.91100   \n",
              "std     288.819436     0.458487  5.812815e+06     0.781416    19.08217   \n",
              "min       1.000000     0.000000  1.886100e+06     0.000000     1.00000   \n",
              "25%     250.750000     0.000000  2.815905e+06     0.000000    10.75000   \n",
              "50%     500.500000     0.000000  4.092145e+06     0.500000    18.00000   \n",
              "75%     750.250000     1.000000  6.545392e+06     1.000000    28.00000   \n",
              "max    1000.000000     1.000000  6.759576e+07     2.000000   155.00000   \n",
              "\n",
              "           1회 평균매출액    할인권 사용 횟수        총 할인 금액         고객등급         구매유형  \\\n",
              "count  1.000000e+03  1000.000000    1000.000000  1000.000000  1000.000000   \n",
              "mean   3.521024e+05    16.027000  292371.670000     1.546000     2.656000   \n",
              "std    3.124636e+05     8.341334  111937.501042     0.498129     1.046307   \n",
              "min    2.708200e+04     1.000000    3750.000000     1.000000     1.000000   \n",
              "25%    1.631242e+05     9.000000  261686.250000     1.000000     2.000000   \n",
              "50%    2.582080e+05    17.000000  347500.000000     2.000000     2.000000   \n",
              "75%    4.268310e+05    23.000000  365400.000000     2.000000     4.000000   \n",
              "max    2.798500e+06    30.000000  400600.000000     2.000000     4.000000   \n",
              "\n",
              "       ...      구매카테고리수         거주지역           성별       고객 나이대         거래기간  \\\n",
              "count  ...  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
              "mean   ...     5.217000     5.147000     0.189000     3.964000  3495.891000   \n",
              "std    ...     2.224153     1.169084     0.391705     1.078827   965.966194   \n",
              "min    ...     1.000000     1.000000     0.000000     2.000000   827.000000   \n",
              "25%    ...     3.000000     4.000000     0.000000     3.000000  2871.000000   \n",
              "50%    ...     5.000000     5.000000     0.000000     4.000000  3836.000000   \n",
              "75%    ...     7.000000     6.000000     0.000000     5.000000  4207.250000   \n",
              "max    ...     9.000000     7.000000     1.000000     7.000000  5334.000000   \n",
              "\n",
              "            할인민감여부      Recency    Frequency     Monetary      평균 구매주기  \n",
              "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
              "mean      0.400000     4.925000     2.289000     4.129000   266.880824  \n",
              "std       0.490143     1.744253     1.669811     1.560383   254.077398  \n",
              "min       0.000000     1.000000     1.000000     1.000000    13.980645  \n",
              "25%       0.000000     4.000000     1.000000     3.000000   111.957671  \n",
              "50%       0.000000     5.000000     2.000000     4.000000   191.469697  \n",
              "75%       1.000000     7.000000     3.000000     6.000000   324.386218  \n",
              "max       1.000000     7.000000     7.000000     7.000000  1956.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-476c881b-a429-436e-8658-7bceabc7e9a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>고객ID</th>\n",
              "      <th>이탈여부</th>\n",
              "      <th>총매출액</th>\n",
              "      <th>구매금액대</th>\n",
              "      <th>방문빈도</th>\n",
              "      <th>1회 평균매출액</th>\n",
              "      <th>할인권 사용 횟수</th>\n",
              "      <th>총 할인 금액</th>\n",
              "      <th>고객등급</th>\n",
              "      <th>구매유형</th>\n",
              "      <th>...</th>\n",
              "      <th>구매카테고리수</th>\n",
              "      <th>거주지역</th>\n",
              "      <th>성별</th>\n",
              "      <th>고객 나이대</th>\n",
              "      <th>거래기간</th>\n",
              "      <th>할인민감여부</th>\n",
              "      <th>Recency</th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Monetary</th>\n",
              "      <th>평균 구매주기</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1.000000e+03</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>5.858013e+06</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>22.91100</td>\n",
              "      <td>3.521024e+05</td>\n",
              "      <td>16.027000</td>\n",
              "      <td>292371.670000</td>\n",
              "      <td>1.546000</td>\n",
              "      <td>2.656000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.217000</td>\n",
              "      <td>5.147000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>3.964000</td>\n",
              "      <td>3495.891000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>4.925000</td>\n",
              "      <td>2.289000</td>\n",
              "      <td>4.129000</td>\n",
              "      <td>266.880824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>288.819436</td>\n",
              "      <td>0.458487</td>\n",
              "      <td>5.812815e+06</td>\n",
              "      <td>0.781416</td>\n",
              "      <td>19.08217</td>\n",
              "      <td>3.124636e+05</td>\n",
              "      <td>8.341334</td>\n",
              "      <td>111937.501042</td>\n",
              "      <td>0.498129</td>\n",
              "      <td>1.046307</td>\n",
              "      <td>...</td>\n",
              "      <td>2.224153</td>\n",
              "      <td>1.169084</td>\n",
              "      <td>0.391705</td>\n",
              "      <td>1.078827</td>\n",
              "      <td>965.966194</td>\n",
              "      <td>0.490143</td>\n",
              "      <td>1.744253</td>\n",
              "      <td>1.669811</td>\n",
              "      <td>1.560383</td>\n",
              "      <td>254.077398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.886100e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>2.708200e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3750.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>827.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.980645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>250.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.815905e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.75000</td>\n",
              "      <td>1.631242e+05</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>261686.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2871.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>111.957671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>500.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.092145e+06</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>18.00000</td>\n",
              "      <td>2.582080e+05</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>347500.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3836.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>191.469697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>750.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.545392e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.00000</td>\n",
              "      <td>4.268310e+05</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>365400.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4207.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>324.386218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.759576e+07</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>155.00000</td>\n",
              "      <td>2.798500e+06</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>400600.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>5334.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1956.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-476c881b-a429-436e-8658-7bceabc7e9a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-476c881b-a429-436e-8658-7bceabc7e9a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-476c881b-a429-436e-8658-7bceabc7e9a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "vE2BSvHTcFK9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#분류예측\n",
        "\n",
        "#변수선택\n",
        "X = df[[\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\", \"구매금액대\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n",
        "\n",
        "#오버샘플링\n",
        "from imblearn.over_sampling import SMOTE #Synthetic minority over-sampling techniqe\n",
        "\n",
        "X_train, Y_train = SMOTE(random_state = 0).fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "Vb9CgjIVc327"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델링\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "z-soUEPVibiP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = MLPClassifier(random_state=0, alpha = 0.001, hidden_layer_sizes = [50])\n",
        "\n",
        "#모형 학습\n",
        "nn_model.fit(X_train, Y_train)\n",
        "\n",
        "#예측\n",
        "Y_pred = nn_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htkFZre9gZpy",
        "outputId": "2bbea086-3b25-451e-a8f5-b3edfa4059d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxVoFS8FhbAe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과값 보고\n",
        "print(\"Y 예측값 \\n\", Y_pred)\n",
        "print(\"accuracy(train) : {:.3f}\".format(nn_model.score(X_train, Y_train)))\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sce7oyqYhIxJ",
        "outputId": "d4a000ed-e3b1-4e37-c12c-b88943b8e906"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값 \n",
            " [1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(train) : 0.889\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91       177\n",
            "           1       0.85      0.89      0.87       123\n",
            "\n",
            "    accuracy                           0.89       300\n",
            "   macro avg       0.89      0.89      0.89       300\n",
            "weighted avg       0.89      0.89      0.89       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = df[df.이탈여부 == 0][\"1회 평균매출액\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n"
      ],
      "metadata": {
        "id": "FmLjRGkkpSgp"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "#모델 생성\n",
        "nn_reg_model = MLPRegressor(random_state = 0, alpha = 1, max_iter = 1000,\n",
        "                            hidden_layer_sizes = [50,50])\n",
        "\n",
        "#모형학습 및 예측\n",
        "nn_reg_model.fit(X_train, Y_train)\n",
        "Y_pred = nn_reg_model.predict(X_test)\n",
        "\n",
        "#결과값 보고\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"train accuracy : {:.3f}\".format(nn_reg_model.score(X_train, Y_train)))\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE : {:.3f}\".format(rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCmDFCuNquFe",
        "outputId": "cb47ce53-aada-4d0d-e692-ecb0c855a349"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [367567.22799569 291986.7171292  286542.39507057 470299.18982825\n",
            " 199015.43022523 314985.16674704 120385.261673    99679.50744792\n",
            " 217207.85032309 321283.5694424  322281.24636508 139031.08090941\n",
            " 172479.30691299 307474.28058998 410674.7673535  282802.14200659\n",
            " 314022.43590656 344082.61534343 266080.18516834 299527.46258462\n",
            " 417918.96528793 213628.82138014 325703.16436337 276841.23973322\n",
            "  83289.89193415 370306.40166324 392255.59988171 148021.81268408\n",
            " 135257.54780785 288250.40948644 329672.79002503 409898.41100752\n",
            " 238134.15585775 368306.05176262 275290.22641391 239081.40629288\n",
            " 266321.69500888 375448.00373975 246605.1330584  641448.06611111\n",
            " 336904.01756403 215252.99856889 241609.33155848 271063.86770618\n",
            " 236907.37454933 382141.50786421 141280.68282403 289125.50608539\n",
            " 298377.15265302 346302.09403242 253397.19897089 508363.2921946\n",
            "  55430.52886374 316398.6845584  307992.35916116 291458.05176895\n",
            " 365830.87156821 337348.7566333  283210.3790915  258622.02741059\n",
            " 321376.87461064 327403.5219942  223207.07109362 339375.24930473\n",
            " 235633.20353306 207937.38459674 262238.13165728 155176.57248646\n",
            " 446908.94216759 158813.06040685 324047.59496836 252640.34325201\n",
            " 249188.03432076 255476.98860462 135204.18190746 486863.31488025\n",
            " 389836.25584169 417264.89960339 137349.62110706 330482.67927447\n",
            " 202882.37492681 244085.71728165 216194.80179372 178187.54506876\n",
            " 326307.74557151 334835.24310067 259056.39725013 318455.2747672\n",
            " 245978.7402989  349977.88045537 313576.00711254 360034.97890785\n",
            " 302180.47800664 346414.147532   237623.24093074 537825.48286654\n",
            " 306899.04560765 198016.87086714 356756.250312   126411.60509786\n",
            " 222199.09532434 293326.26886908 357988.60770913 274905.50147079\n",
            " 321045.37222521 308870.73491254 328625.82189502 200975.70277517\n",
            " 296192.52973971 279928.30765362 260548.00636601 243793.66083518\n",
            " 342931.62784114 486251.93814507 263093.75393189 222741.17780968\n",
            " 280110.46803518 314221.44235756 237481.55329132 271619.53638204\n",
            " 356806.58975663 367708.28247153 232120.08009347 279820.78777821\n",
            " 336140.0816819  301035.34267013 196964.58034718 242703.54496087\n",
            " 382656.27941644 264746.6531743   74865.40367927 229185.42533768\n",
            " 449451.21775471 333497.22792232 181490.12700764 308568.97923415\n",
            " 162126.67867317 273979.725792   312484.58403589 268972.60081153\n",
            " 294422.26870008 280549.89684759 369158.98151701 236824.03178162\n",
            " 191405.47832004 351443.85315144 298057.17180239 366323.17194185\n",
            " 160087.98542242 289201.47993591 187648.53842493 275254.96461994\n",
            " 296878.85455016 351165.94366769 685012.47921711 197831.35625512\n",
            "    817.42017899 336795.26572669 307319.33036176 286538.74627755\n",
            " 356601.98981613 413427.13794709 399347.35085261 181438.83136968\n",
            " 229242.10355842 330888.72112284 326911.57257259 213774.88283495\n",
            " 372421.41967877 243911.48617397   1134.30582295 251342.72805545\n",
            " 149121.8720192  518450.65557447 163722.93239689 330273.5210797\n",
            " 628305.94584286 229238.61156581 308547.9272714  150580.26882881\n",
            " 557029.72398692 782868.63100081 389620.35786206 424001.85956249\n",
            " 261503.41228481 201832.48696299 311830.67263387 308643.85612775\n",
            "  63366.06569213 343396.35371872 309710.08763841 315943.40002105\n",
            " 590729.84011205  81729.53183386 282024.21237733 289056.00731604\n",
            " 329795.69550524 107196.24435223 373365.59284941 119783.54874409\n",
            " 474729.87515126 108723.20464901 294099.46394641 263585.28216134\n",
            " 282962.56218265 223533.52354009 317057.78433315 328600.46139415\n",
            " 342669.79765667 293334.82175315]\n",
            "train accuracy : 0.300\n",
            "RMSE : 227578.429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#딥러닝\n",
        "##경고문 제거"
      ],
      "metadata": {
        "id": "ZR0l7ZTAuRb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "E-jGhMxHuaH5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[[\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\", \"구매금액대\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n",
        "\n",
        "#오버샘플링\n",
        "from imblearn.over_sampling import SMOTE #Synthetic minority over-sampling techniqe\n",
        "\n",
        "X_train, Y_train = SMOTE(random_state = 0).fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "wRYPEEjjueRe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential  #노드에 레이어가 생기는 데 신경망에는 레이어가 있다.\n",
        "#그리고 노드가 있는데 이런 레이더들이 이어져서 아웃풋으로 이어진다.\n",
        "#시퀀셜을 순차적으로 가는 것을 만드는 것이다.\n",
        "from keras.layers import Dense, Activation#dense는 노드들의 수이다.\n",
        "#노드의 밀도를 정해주는데 밀도가 높다는 것은 그 중에서 계산이 많다는 것이다.\n",
        "from keras.metrics import Accuracy\n",
        "\n",
        "#시드값 설정\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "#모형생성\n",
        "model = keras.models.Sequential()#순차적인 모형을 형성\n",
        "model.add(keras.layers.Dense(64, input_dim=7, activation = \"relu\")) #레이어나 밀도를 추가한다.\n",
        "#input dim= 7 -> 7차원의 신경망을 만들겠다. -> 밀도는 64이다. 하이퍼파라미터 지정 활성화함수 (activation)\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "#모형학습\n",
        "#불순도 확인 모델이 얼마나 불순도를 없앴는지, 불순도는 적합성과 반비례관계를 가진다.\n",
        "#메트릭스는 적합도를 보여준다\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_split = 0.2, epochs = 100, \n",
        "                    batch_size = 64, verbose = 2)\n",
        "#변수를 지정하기 위해서 지정 두개의 훈련셋을 넣어서 100번의 계산을 시도한다.\n",
        "#1부터 시작 - 100까지 계산한다. 대신 계산이 오래걸림\n",
        "#벤치 사이즈와 벌보스는 그냥 디폴트값으로 사용하면 된다. \n",
        "#적정한 에폭을 결정하는것이 중요하다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo4XAUjMupsU",
        "outputId": "aa781d8f-d8a7-4f5d-9dab-5317447c8d92"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 2s - loss: 0.6829 - accuracy: 0.5888 - val_loss: 0.6856 - val_accuracy: 0.6529 - 2s/epoch - 159ms/step\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.5836 - accuracy: 0.8254 - val_loss: 0.6330 - val_accuracy: 0.6471 - 87ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.5174 - accuracy: 0.8284 - val_loss: 0.6003 - val_accuracy: 0.6765 - 79ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.4679 - accuracy: 0.8328 - val_loss: 0.5588 - val_accuracy: 0.7353 - 88ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.4343 - accuracy: 0.8595 - val_loss: 0.5102 - val_accuracy: 0.8118 - 84ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.4104 - accuracy: 0.8624 - val_loss: 0.5205 - val_accuracy: 0.7882 - 150ms/epoch - 14ms/step\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.3902 - accuracy: 0.8713 - val_loss: 0.4881 - val_accuracy: 0.8235 - 97ms/epoch - 9ms/step\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.3756 - accuracy: 0.8802 - val_loss: 0.4970 - val_accuracy: 0.8176 - 114ms/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.3606 - accuracy: 0.8905 - val_loss: 0.4988 - val_accuracy: 0.8235 - 137ms/epoch - 12ms/step\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.3481 - accuracy: 0.8876 - val_loss: 0.4776 - val_accuracy: 0.8412 - 158ms/epoch - 14ms/step\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.3354 - accuracy: 0.8964 - val_loss: 0.5106 - val_accuracy: 0.8235 - 183ms/epoch - 17ms/step\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.3230 - accuracy: 0.8994 - val_loss: 0.4591 - val_accuracy: 0.8529 - 159ms/epoch - 14ms/step\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.3110 - accuracy: 0.9038 - val_loss: 0.4816 - val_accuracy: 0.8471 - 221ms/epoch - 20ms/step\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.3008 - accuracy: 0.9038 - val_loss: 0.4843 - val_accuracy: 0.8471 - 183ms/epoch - 17ms/step\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.2923 - accuracy: 0.9112 - val_loss: 0.4878 - val_accuracy: 0.8529 - 121ms/epoch - 11ms/step\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.2855 - accuracy: 0.9172 - val_loss: 0.4796 - val_accuracy: 0.8529 - 130ms/epoch - 12ms/step\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.2792 - accuracy: 0.9172 - val_loss: 0.4837 - val_accuracy: 0.8588 - 85ms/epoch - 8ms/step\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.2729 - accuracy: 0.9216 - val_loss: 0.4902 - val_accuracy: 0.8588 - 98ms/epoch - 9ms/step\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.2664 - accuracy: 0.9157 - val_loss: 0.4760 - val_accuracy: 0.8588 - 85ms/epoch - 8ms/step\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.2629 - accuracy: 0.9186 - val_loss: 0.4975 - val_accuracy: 0.8588 - 82ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.2586 - accuracy: 0.9216 - val_loss: 0.4753 - val_accuracy: 0.8529 - 132ms/epoch - 12ms/step\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.2545 - accuracy: 0.9216 - val_loss: 0.4969 - val_accuracy: 0.8529 - 138ms/epoch - 13ms/step\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.2500 - accuracy: 0.9216 - val_loss: 0.4686 - val_accuracy: 0.8588 - 137ms/epoch - 12ms/step\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4818 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.2481 - accuracy: 0.9186 - val_loss: 0.4760 - val_accuracy: 0.8588 - 81ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.2421 - accuracy: 0.9201 - val_loss: 0.4532 - val_accuracy: 0.8588 - 96ms/epoch - 9ms/step\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.2418 - accuracy: 0.9246 - val_loss: 0.4800 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.2370 - accuracy: 0.9260 - val_loss: 0.4627 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.2360 - accuracy: 0.9172 - val_loss: 0.4610 - val_accuracy: 0.8588 - 62ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.2324 - accuracy: 0.9275 - val_loss: 0.4712 - val_accuracy: 0.8529 - 44ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.2309 - accuracy: 0.9290 - val_loss: 0.4492 - val_accuracy: 0.8588 - 110ms/epoch - 10ms/step\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.2293 - accuracy: 0.9290 - val_loss: 0.4589 - val_accuracy: 0.8588 - 101ms/epoch - 9ms/step\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.2291 - accuracy: 0.9305 - val_loss: 0.4483 - val_accuracy: 0.8588 - 104ms/epoch - 9ms/step\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.2252 - accuracy: 0.9290 - val_loss: 0.4475 - val_accuracy: 0.8588 - 144ms/epoch - 13ms/step\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.2247 - accuracy: 0.9290 - val_loss: 0.4419 - val_accuracy: 0.8588 - 74ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.2240 - accuracy: 0.9246 - val_loss: 0.4333 - val_accuracy: 0.8647 - 81ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.2223 - accuracy: 0.9305 - val_loss: 0.4677 - val_accuracy: 0.8529 - 81ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.2193 - accuracy: 0.9290 - val_loss: 0.4230 - val_accuracy: 0.8588 - 63ms/epoch - 6ms/step\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.2183 - accuracy: 0.9305 - val_loss: 0.4455 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.2165 - accuracy: 0.9334 - val_loss: 0.4343 - val_accuracy: 0.8647 - 43ms/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.2149 - accuracy: 0.9334 - val_loss: 0.4333 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.2139 - accuracy: 0.9320 - val_loss: 0.4404 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.2145 - accuracy: 0.9260 - val_loss: 0.4200 - val_accuracy: 0.8588 - 81ms/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.2120 - accuracy: 0.9349 - val_loss: 0.4548 - val_accuracy: 0.8529 - 133ms/epoch - 12ms/step\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.2110 - accuracy: 0.9334 - val_loss: 0.4268 - val_accuracy: 0.8647 - 84ms/epoch - 8ms/step\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.2082 - accuracy: 0.9364 - val_loss: 0.4381 - val_accuracy: 0.8529 - 87ms/epoch - 8ms/step\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.2085 - accuracy: 0.9364 - val_loss: 0.4311 - val_accuracy: 0.8588 - 43ms/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.2071 - accuracy: 0.9349 - val_loss: 0.4126 - val_accuracy: 0.8647 - 44ms/epoch - 4ms/step\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.2064 - accuracy: 0.9379 - val_loss: 0.4292 - val_accuracy: 0.8588 - 41ms/epoch - 4ms/step\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.2051 - accuracy: 0.9334 - val_loss: 0.4113 - val_accuracy: 0.8647 - 67ms/epoch - 6ms/step\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.2056 - accuracy: 0.9379 - val_loss: 0.4268 - val_accuracy: 0.8588 - 75ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.2044 - accuracy: 0.9349 - val_loss: 0.4156 - val_accuracy: 0.8647 - 94ms/epoch - 9ms/step\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.2026 - accuracy: 0.9364 - val_loss: 0.4326 - val_accuracy: 0.8529 - 63ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.2007 - accuracy: 0.9408 - val_loss: 0.4113 - val_accuracy: 0.8647 - 47ms/epoch - 4ms/step\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9393 - val_loss: 0.4058 - val_accuracy: 0.8647 - 47ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.1999 - accuracy: 0.9393 - val_loss: 0.4358 - val_accuracy: 0.8588 - 90ms/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.1996 - accuracy: 0.9393 - val_loss: 0.3937 - val_accuracy: 0.8647 - 42ms/epoch - 4ms/step\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.1997 - accuracy: 0.9364 - val_loss: 0.4277 - val_accuracy: 0.8529 - 55ms/epoch - 5ms/step\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.1975 - accuracy: 0.9408 - val_loss: 0.3932 - val_accuracy: 0.8647 - 60ms/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.1948 - accuracy: 0.9423 - val_loss: 0.4205 - val_accuracy: 0.8588 - 50ms/epoch - 5ms/step\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.1951 - accuracy: 0.9438 - val_loss: 0.4008 - val_accuracy: 0.8647 - 85ms/epoch - 8ms/step\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.1935 - accuracy: 0.9408 - val_loss: 0.4069 - val_accuracy: 0.8647 - 47ms/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.1947 - accuracy: 0.9423 - val_loss: 0.4142 - val_accuracy: 0.8588 - 42ms/epoch - 4ms/step\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.1929 - accuracy: 0.9438 - val_loss: 0.4039 - val_accuracy: 0.8647 - 44ms/epoch - 4ms/step\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.1912 - accuracy: 0.9453 - val_loss: 0.4085 - val_accuracy: 0.8647 - 72ms/epoch - 7ms/step\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.1911 - accuracy: 0.9438 - val_loss: 0.4082 - val_accuracy: 0.8647 - 46ms/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.1901 - accuracy: 0.9438 - val_loss: 0.4114 - val_accuracy: 0.8588 - 49ms/epoch - 4ms/step\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.1910 - accuracy: 0.9408 - val_loss: 0.3896 - val_accuracy: 0.8647 - 52ms/epoch - 5ms/step\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.1892 - accuracy: 0.9453 - val_loss: 0.3962 - val_accuracy: 0.8647 - 69ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.1887 - accuracy: 0.9408 - val_loss: 0.4095 - val_accuracy: 0.8588 - 47ms/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.1872 - accuracy: 0.9438 - val_loss: 0.3883 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.1875 - accuracy: 0.9408 - val_loss: 0.3898 - val_accuracy: 0.8647 - 45ms/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.1867 - accuracy: 0.9482 - val_loss: 0.4074 - val_accuracy: 0.8588 - 65ms/epoch - 6ms/step\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.1880 - accuracy: 0.9393 - val_loss: 0.3759 - val_accuracy: 0.8647 - 120ms/epoch - 11ms/step\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.1884 - accuracy: 0.9438 - val_loss: 0.4138 - val_accuracy: 0.8588 - 46ms/epoch - 4ms/step\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.1838 - accuracy: 0.9423 - val_loss: 0.3702 - val_accuracy: 0.8647 - 48ms/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.1842 - accuracy: 0.9467 - val_loss: 0.3963 - val_accuracy: 0.8588 - 48ms/epoch - 4ms/step\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.1831 - accuracy: 0.9497 - val_loss: 0.3934 - val_accuracy: 0.8588 - 45ms/epoch - 4ms/step\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.1830 - accuracy: 0.9467 - val_loss: 0.3960 - val_accuracy: 0.8588 - 72ms/epoch - 7ms/step\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.1824 - accuracy: 0.9423 - val_loss: 0.3860 - val_accuracy: 0.8647 - 118ms/epoch - 11ms/step\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.1808 - accuracy: 0.9482 - val_loss: 0.3950 - val_accuracy: 0.8588 - 68ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.1814 - accuracy: 0.9482 - val_loss: 0.3916 - val_accuracy: 0.8588 - 76ms/epoch - 7ms/step\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.1797 - accuracy: 0.9497 - val_loss: 0.3803 - val_accuracy: 0.8647 - 84ms/epoch - 8ms/step\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.1793 - accuracy: 0.9467 - val_loss: 0.3954 - val_accuracy: 0.8588 - 100ms/epoch - 9ms/step\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.1794 - accuracy: 0.9453 - val_loss: 0.3924 - val_accuracy: 0.8588 - 88ms/epoch - 8ms/step\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.1789 - accuracy: 0.9482 - val_loss: 0.3679 - val_accuracy: 0.8588 - 83ms/epoch - 8ms/step\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.1777 - accuracy: 0.9497 - val_loss: 0.3876 - val_accuracy: 0.8588 - 136ms/epoch - 12ms/step\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.1773 - accuracy: 0.9512 - val_loss: 0.3835 - val_accuracy: 0.8588 - 82ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.1784 - accuracy: 0.9438 - val_loss: 0.3948 - val_accuracy: 0.8588 - 85ms/epoch - 8ms/step\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.1765 - accuracy: 0.9527 - val_loss: 0.3870 - val_accuracy: 0.8588 - 137ms/epoch - 12ms/step\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.1758 - accuracy: 0.9497 - val_loss: 0.3666 - val_accuracy: 0.8647 - 88ms/epoch - 8ms/step\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.1772 - accuracy: 0.9423 - val_loss: 0.3699 - val_accuracy: 0.8588 - 132ms/epoch - 12ms/step\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.1755 - accuracy: 0.9512 - val_loss: 0.3941 - val_accuracy: 0.8588 - 96ms/epoch - 9ms/step\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.1753 - accuracy: 0.9467 - val_loss: 0.3659 - val_accuracy: 0.8588 - 94ms/epoch - 9ms/step\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9512 - val_loss: 0.3836 - val_accuracy: 0.8588 - 82ms/epoch - 7ms/step\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.1743 - accuracy: 0.9482 - val_loss: 0.3720 - val_accuracy: 0.8588 - 138ms/epoch - 13ms/step\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.1739 - accuracy: 0.9527 - val_loss: 0.3804 - val_accuracy: 0.8588 - 111ms/epoch - 10ms/step\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.1749 - accuracy: 0.9423 - val_loss: 0.3558 - val_accuracy: 0.8647 - 106ms/epoch - 10ms/step\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.1722 - accuracy: 0.9482 - val_loss: 0.3841 - val_accuracy: 0.8588 - 70ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.1716 - accuracy: 0.9497 - val_loss: 0.3719 - val_accuracy: 0.8588 - 57ms/epoch - 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#에폭에 나왔단 로스랑 어큐러씨를 그려본다.(크로스 되는 지점 확인)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig,loss_ax = plt.subplots()\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "#loss(오차) 그리기\n",
        "loss_ax.plot(history.history[\"loss\"], \"y\", label = \"train loss\")\n",
        "loss_ax.plot(history.history[\"val_loss\"], \"r\", label = \"val_loss\")\n",
        "loss_ax.set_xlabel(\"epoch\")\n",
        "loss_ax.set_ylabel(\"loss\")\n",
        "loss_ax.legend(loc = \"lower right\") #legend 는 범례 \n",
        "\n",
        "#accuracy(정확도) 그리기\n",
        "acc_ax.plot(history.history[\"accuracy\"], \"b\", label = \"train acc\")\n",
        "acc_ax.plot(history.history[\"val_accuracy\"], \"g\", label = \"val_acc\")\n",
        "acc_ax.set_ylabel(\"accuracy\")\n",
        "acc_ax.legend(loc = \"upper right\") #legend 는 범례 \n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "dLpD6Z2K4arZ",
        "outputId": "42dd88ae-4286-46cf-9271-df3297a7949d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, validation_split = 0.2, epochs = 20, batch_size = 64, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23U7fH5c7kBL",
        "outputId": "434469ca-dca2-408e-a2a9-9bf558883a48"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1702 - accuracy: 0.9497 - val_loss: 0.3680 - val_accuracy: 0.8588\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1697 - accuracy: 0.9512 - val_loss: 0.3609 - val_accuracy: 0.8588\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9497 - val_loss: 0.3671 - val_accuracy: 0.8588\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1696 - accuracy: 0.9497 - val_loss: 0.3579 - val_accuracy: 0.8647\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9512 - val_loss: 0.3734 - val_accuracy: 0.8588\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9512 - val_loss: 0.3627 - val_accuracy: 0.8588\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9527 - val_loss: 0.3614 - val_accuracy: 0.8588\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1687 - accuracy: 0.9527 - val_loss: 0.3717 - val_accuracy: 0.8588\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1661 - accuracy: 0.9512 - val_loss: 0.3677 - val_accuracy: 0.8588\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1667 - accuracy: 0.9512 - val_loss: 0.3754 - val_accuracy: 0.8588\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1667 - accuracy: 0.9512 - val_loss: 0.3679 - val_accuracy: 0.8588\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9527 - val_loss: 0.3560 - val_accuracy: 0.8588\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1645 - accuracy: 0.9512 - val_loss: 0.3706 - val_accuracy: 0.8588\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1645 - accuracy: 0.9512 - val_loss: 0.3531 - val_accuracy: 0.8588\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.1635 - accuracy: 0.9512 - val_loss: 0.3723 - val_accuracy: 0.8588\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1644 - accuracy: 0.9527 - val_loss: 0.3496 - val_accuracy: 0.8588\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9497 - val_loss: 0.3620 - val_accuracy: 0.8588\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.9497 - val_loss: 0.3627 - val_accuracy: 0.8588\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1621 - accuracy: 0.9527 - val_loss: 0.3453 - val_accuracy: 0.8647\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9527 - val_loss: 0.3684 - val_accuracy: 0.8588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7673ba22b0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred  = model.predict(X_test, verbose = 0)\n",
        "Y_class = np.round(Y_pred, 0)\n",
        "train_score = model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Y 예측값 : \\n\", Y_pred[:5])\n",
        "print(\"Y 예측 클래스 : \\n \", Y_class[:5] )\n",
        "print(\"train accuracy : {:.3f}\".format(train_score[0], train_score[1]))\n",
        "print(\"test accuracy : {:.3f}\".format(test_score[0], test_score[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzpzjDf87sMJ",
        "outputId": "d6c81bbc-87ac-4e83-877c-d4fa486c252f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y 예측값 : \n",
            " [[0.93126047]\n",
            " [1.        ]\n",
            " [0.01772536]\n",
            " [0.01244921]\n",
            " [0.99973166]]\n",
            "Y 예측 클래스 : \n",
            "  [[1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n",
            "train accuracy : 0.202\n",
            "test accuracy : 0.224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#수치예측"
      ],
      "metadata": {
        "id": "BEDE0NKg7o7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK78sew3VPBw",
        "outputId": "e5adfcab-19b2-4ec6-93b4-a31b81cbe5ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "c24IxVs8Vgn0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df[df.이탈여부==0][[\"방문빈도\", \"총 할인 금액\", \"구매카테고리수\", \"거래기간\"]]\n",
        "Y=np.log1p(df[df.이탈여부==0][\"1회 평균매출액\"])\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "scalar = StandardScaler().fit(X_train)\n",
        "X_train= scalar.transform(X_train)\n",
        "X_test= scalar.transform(X_test)\n",
        "\n",
        "#seed값 설정\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "#모형생성\n",
        "model = keras.models.Sequential()#순차적인 모형을 형성\n",
        "model.add(keras.layers.Dense(64, input_dim=4, activation = \"relu\")) #레이어나 밀도를 추가한다.\n",
        "#input dim= 7 -> 7차원의 신경망을 만들겠다. -> 밀도는 64이다. 하이퍼파라미터 지정 활성화함수 (activation)\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "\n",
        "#모형학습\n",
        "#불순도 확인 모델이 얼마나 불순도를 없앴는지, 불순도는 적합성과 반비례관계를 가진다.\n",
        "#메트릭스는 적합도를 보여준다\n",
        "model.compile(loss = \"mse\", optimizer = \"SGD\")\n",
        "Y_pred = np.round(model.predict(X_test[:5], verbose=0), 3)\n",
        "print(\"Y predict value \\n\", Y_pred)\n",
        "\n",
        "#모형 평가\n",
        "train_score = model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"train mse : {:.3f}\".format(train_score))\n",
        "print(\"test mse : {:.3f}\".format(test_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cpBlA8XVULS",
        "outputId": "ed0bc63a-04fb-4be6-e3a8-d30f364c1149"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [[0.    0.    0.    0.234 0.    0.163 0.052 0.    0.    0.    0.    0.32\n",
            "  0.013 0.    0.218 0.009 0.111 0.    0.182 0.    0.    0.    0.474 0.143\n",
            "  0.    0.    0.    0.    0.    0.    0.325 0.    0.    0.224 0.    0.098\n",
            "  0.115 0.    0.    0.055 0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.322 0.12  0.098 0.    0.336 0.142 0.    0.    0.394 0.    0.498\n",
            "  0.    0.    0.182 0.   ]\n",
            " [0.    0.    0.    0.002 0.    0.134 0.114 0.    0.109 0.    0.    0.058\n",
            "  0.061 0.043 0.042 0.027 0.06  0.097 0.114 0.    0.    0.001 0.14  0.\n",
            "  0.081 0.017 0.    0.034 0.    0.    0.012 0.009 0.    0.045 0.    0.039\n",
            "  0.    0.    0.    0.109 0.    0.009 0.004 0.    0.    0.    0.    0.\n",
            "  0.059 0.097 0.074 0.026 0.    0.014 0.184 0.    0.002 0.061 0.075 0.146\n",
            "  0.    0.    0.289 0.   ]\n",
            " [0.    0.    0.079 0.012 0.    0.056 0.061 0.    0.175 0.    0.089 0.14\n",
            "  0.002 0.    0.254 0.04  0.094 0.183 0.181 0.    0.058 0.    0.385 0.058\n",
            "  0.    0.    0.    0.057 0.    0.    0.144 0.051 0.07  0.104 0.023 0.032\n",
            "  0.036 0.    0.    0.162 0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.054 0.124 0.055 0.014 0.    0.081 0.055 0.    0.057 0.15  0.048 0.408\n",
            "  0.    0.    0.363 0.   ]\n",
            " [0.    0.002 0.    0.121 0.    0.06  0.097 0.115 0.    0.    0.232 0.095\n",
            "  0.    0.    0.11  0.    0.147 0.153 0.168 0.    0.    0.    0.268 0.204\n",
            "  0.    0.    0.    0.    0.    0.    0.448 0.    0.    0.254 0.    0.075\n",
            "  0.214 0.    0.    0.004 0.043 0.    0.024 0.    0.    0.    0.036 0.\n",
            "  0.    0.219 0.    0.2   0.    0.357 0.169 0.    0.134 0.232 0.016 0.495\n",
            "  0.    0.    0.222 0.   ]\n",
            " [0.    0.    0.    0.024 0.018 0.053 0.087 0.088 0.054 0.    0.102 0.003\n",
            "  0.    0.    0.068 0.    0.    0.145 0.005 0.    0.    0.    0.111 0.075\n",
            "  0.    0.096 0.    0.    0.    0.    0.013 0.    0.    0.064 0.    0.094\n",
            "  0.166 0.    0.    0.    0.    0.    0.    0.    0.014 0.    0.06  0.\n",
            "  0.    0.132 0.    0.171 0.    0.028 0.094 0.    0.    0.    0.    0.068\n",
            "  0.    0.    0.053 0.   ]]\n",
            "train mse : 152.833\n",
            "test mse : 152.401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#앙상블\n"
      ],
      "metadata": {
        "id": "KaLpSVuZgZJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGBYc-KzMgtd",
        "outputId": "b91da026-aa9a-4040-d254-bb22f7f9b619"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 21 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   고객ID       1000 non-null   int64  \n",
            " 1   이탈여부       1000 non-null   int64  \n",
            " 2   총매출액       1000 non-null   int64  \n",
            " 3   구매금액대      1000 non-null   int64  \n",
            " 4   방문빈도       1000 non-null   int64  \n",
            " 5   1회 평균매출액   1000 non-null   int64  \n",
            " 6   할인권 사용 횟수  1000 non-null   int64  \n",
            " 7   총 할인 금액    1000 non-null   int64  \n",
            " 8   고객등급       1000 non-null   int64  \n",
            " 9   구매유형       1000 non-null   int64  \n",
            " 10  클레임접수여부    1000 non-null   int64  \n",
            " 11  구매카테고리수    1000 non-null   int64  \n",
            " 12  거주지역       1000 non-null   int64  \n",
            " 13  성별         1000 non-null   int64  \n",
            " 14  고객 나이대     1000 non-null   int64  \n",
            " 15  거래기간       1000 non-null   int64  \n",
            " 16  할인민감여부     1000 non-null   int64  \n",
            " 17  Recency    1000 non-null   int64  \n",
            " 18  Frequency  1000 non-null   int64  \n",
            " 19  Monetary   1000 non-null   int64  \n",
            " 20  평균 구매주기    1000 non-null   float64\n",
            "dtypes: float64(1), int64(20)\n",
            "memory usage: 164.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n",
        "\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "lz4DeRkSj2ca"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################33\n",
        "######################으억\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state = 0)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "#앙상블 모형 생성\n",
        "model = VotingClassifier(estimators = [(\"K-NN\", knn), (\"Dtree\", dtree)], voting = \"soft\")\n",
        "\n",
        "#모형학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#예측 및 모형 성능 평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "#8-1 보팅모형 정확도\n",
        "print(\"voting classifier accuracy : {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "#8-2 개별모형 정확도\n",
        "classifiers = [dtree, knn]\n",
        "for classifier in classifiers :\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  class_name = classifier.__class__.__name__\n",
        "  print(\"{0} accuracy : {1:3f}\". format(class_name, classifier.score(X_test, Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "872vOThSiZSQ",
        "outputId": "6ed41068-4c5c-4f17-efff-13e3312aa870"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0]\n",
            "voting classifier accuracy : 0.907\n",
            "DecisionTreeClassifier accuracy : 0.900000\n",
            "KNeighborsClassifier accuracy : 0.850000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)"
      ],
      "metadata": {
        "id": "6ff8O2z0oX6s"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svr=SVR()\n",
        "mlp=MLPRegressor(random_state=0)\n",
        "\n",
        "#5 voting model 생성\n",
        "model = VotingRegressor(estimators = [(\"SVR\", svr), (\"MLP\", mlp)])\n",
        "\n",
        "#6 모형 학습\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "#7 예측 및 평가\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value \\n\", Y_pred)\n",
        "#8-1 보팅모형 정확도\n",
        "print(\"voting regressor accuracy : {0:.3f}\".format(model.score(X_test, Y_test)))\n",
        "#8-2 개별모형 정확도\n",
        "Regressors = [svr, mlp]\n",
        "for Regressors in Regressors :\n",
        "  Regressors.fit(X_train, Y_train)\n",
        "  class_name = Regressors.__class__.__name__\n",
        "  print(\"{0} accuracy : {1:3f}\". format(class_name, Regressors.score(X_test, Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUuJ4CsBoipS",
        "outputId": "75babaeb-93b7-4a66-9e90-070e52029ef9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value \n",
            " [12.72637282 12.15785517 12.09686124 13.26831502 11.60231958 12.47233857\n",
            " 11.00403584 12.11236139 11.92418038 12.36295705 12.33471768 11.93634366\n",
            " 12.10248328 12.17485959 12.54047237 12.21174042 12.79796392 12.85868158\n",
            " 12.14723369 12.51820132 12.73655925 11.51053651 12.16572772 12.61967869\n",
            " 10.97450508 13.11252037 12.5792913  12.13627155 12.04792775 11.98604929\n",
            " 12.79524713 12.95793432 12.51574594 13.14826771 12.21608558 11.91077705\n",
            " 11.84798695 12.99567742 11.95362493 13.66455063 12.85068225 12.1427706\n",
            " 12.49680643 12.12413092 12.41795186 12.43119996 11.29956398 12.14660018\n",
            " 12.69704459 12.43998557 12.01109351 13.29011823 11.1046136  12.92296162\n",
            " 12.8522466  12.08692287 12.46809297 12.34155171 12.26251834 12.06954504\n",
            " 12.87142564 12.96736873 11.66603809 12.35227513 12.06159606 12.155967\n",
            " 12.10647664 11.48482286 12.69218789 11.79509489 12.20279207 11.67227129\n",
            " 11.93748831 11.75242257 12.08799173 12.79011812 12.43771399 12.93671652\n",
            " 12.21409548 12.39436336 12.71746922 11.95532413 11.82312984 12.16287274\n",
            " 12.28686047 12.69164266 12.44251859 12.77121709 11.83604445 12.83491032\n",
            " 12.14491251 13.21743021 12.50243799 13.06892645 11.92415719 13.03858984\n",
            " 12.88053564 12.160099   12.91472933 11.63221781 11.94370304 12.44004892\n",
            " 12.87464916 11.8729167  12.3654078  12.55856624 12.84071252 11.71225848\n",
            " 12.26380165 12.45982361 12.02043293 11.9421507  12.73253879 13.02635162\n",
            " 12.04973161 12.33438799 12.41827333 12.86241083 12.40875392 12.5968783\n",
            " 12.81779588 12.8274637  12.19010157 12.59904112 12.84774747 12.18532332\n",
            " 12.22778031 11.8054232  13.12475682 11.95865789 11.81489928 11.55897355\n",
            " 13.07522943 12.28451019 11.8789114  12.15561689 11.51884822 12.18398225\n",
            " 12.69227703 11.85075753 12.26168017 12.16159193 12.65547847 11.98416886\n",
            " 12.1825295  12.53395352 12.63117748 12.89234566 12.03663241 12.06490339\n",
            " 12.17004133 12.65506435 12.64528232 12.859696   13.98345371 11.56676754\n",
            " 12.14814582 12.76169623 12.08281248 12.78057339 12.92206256 12.64509877\n",
            " 12.47824953 11.8252805  12.39300142 12.55519649 12.769953   11.36679481\n",
            " 12.96480349 12.50216886 11.50980876 11.95884547 11.44958371 13.16010026\n",
            " 12.05559067 12.71797611 13.99850887 11.871165   12.31221069 11.61007956\n",
            " 13.80229617 14.53772869 12.52183126 12.69249083 12.21280298 11.72885039\n",
            " 12.22192476 12.20813083 11.39735772 12.84417613 12.23391116 12.77976915\n",
            " 13.54789146 11.70010427 12.13965267 12.01776146 12.34314015 12.29642101\n",
            " 12.7601278  11.25118371 13.264752   11.77360757 12.25216632 11.80519914\n",
            " 12.70641052 11.83457641 12.32938499 12.20461754 12.97324333 12.52229619]\n",
            "voting regressor accuracy : 0.538\n",
            "SVR accuracy : 0.608035\n",
            "MLPRegressor accuracy : 0.224841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#URL 쓰기\n",
        "\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "\n",
        "client_id = \"U6T2yf5vdHviz6u5luir\"\n",
        "client_secret = \"2lFmRI34fo\"\n",
        "\n",
        "\n",
        "url = \"https://openapi.naver.com/v1/datalab/search\"\n",
        "body = \"{\\\n",
        "         \\\"startDate\\\":\\\"2022-10-01\\\",\\\n",
        "         \\\"endDate\\\":\\\"2023-01-01\\\",\\\n",
        "         \\\"timeUnit\\\":\\\"date\\\",\\\n",
        "         \\\"keywordGroups\\\":[{\\\"groupName\\\":\\\"유비온\\\",\\\"keywords\\\":[\\\"유비온\\\",\\\"korean\\\"]},\\\n",
        "                             {\\\"groupName\\\":\\\"영어\\\",\\\"keywords\\\":[\\\"영어\\\",\\\"english\\\"]}\\\n",
        "                            ],\\\n",
        "         \\\"device\\\":\\\"pc\\\",\\\n",
        "         \\\"ages\\\":[\\\"1\\\",\\\"2\\\"],\\\n",
        "         \\\"gender\\\":\\\"f\\\"\\\n",
        "         }\";\n",
        "\n",
        "request = urllib.request.Request(url)\n",
        "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "request.add_header(\"Content-Type\",\"application/json\")\n",
        "\n",
        "response = urllib.request.urlopen(request, data=body.encode(\"utf-8\"))\n",
        "\n",
        "rescode = response.getcode()\n",
        "\n",
        "if(rescode==200):\n",
        "    response_body = response.read()\n",
        "    response_data = response_body.decode('utf-8')\n",
        "else:\n",
        "    print(\"Error Code:\" + rescode)\n",
        "\n",
        "result = json.loads(response_data)\n",
        "\n",
        "print(result)\n",
        "\n",
        "date = [a['period'] for a in result['results'][0]['data']]\n",
        "ratio_data1 = [a['ratio'] for a in result['results'][0]['data']]\n",
        "ratio_data2 = [a['ratio'] for a in result['results'][1]['data']]\n",
        "\n",
        "pd.DataFrame({'date':date,\n",
        "        'seraching_result':ratio_data1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w-LbOpispIA6",
        "outputId": "33a6f135-9e25-4007-ea6c-8616f2883df0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'startDate': '2022-10-01', 'endDate': '2023-01-01', 'timeUnit': 'date', 'results': [{'title': '유비온', 'keywords': ['유비온', 'korean'], 'data': [{'period': '2022-10-02', 'ratio': 2}, {'period': '2022-10-03', 'ratio': 6}, {'period': '2022-10-04', 'ratio': 2}, {'period': '2022-10-05', 'ratio': 8}, {'period': '2022-10-10', 'ratio': 2}, {'period': '2022-10-11', 'ratio': 4}, {'period': '2022-10-13', 'ratio': 4}, {'period': '2022-10-15', 'ratio': 4}, {'period': '2022-10-16', 'ratio': 2}, {'period': '2022-10-17', 'ratio': 2}, {'period': '2022-10-18', 'ratio': 2}, {'period': '2022-10-21', 'ratio': 2}, {'period': '2022-10-25', 'ratio': 2}, {'period': '2022-10-26', 'ratio': 4}, {'period': '2022-10-28', 'ratio': 2}, {'period': '2022-10-31', 'ratio': 4}, {'period': '2022-11-02', 'ratio': 4}, {'period': '2022-11-05', 'ratio': 2}, {'period': '2022-11-07', 'ratio': 6}, {'period': '2022-11-09', 'ratio': 4}, {'period': '2022-11-11', 'ratio': 2}, {'period': '2022-11-14', 'ratio': 4}, {'period': '2022-11-16', 'ratio': 4}, {'period': '2022-11-17', 'ratio': 2}, {'period': '2022-11-18', 'ratio': 2}, {'period': '2022-11-19', 'ratio': 2}, {'period': '2022-11-20', 'ratio': 2}, {'period': '2022-11-22', 'ratio': 2}, {'period': '2022-11-26', 'ratio': 2}, {'period': '2022-11-27', 'ratio': 2}, {'period': '2022-11-28', 'ratio': 4}, {'period': '2022-11-30', 'ratio': 4}, {'period': '2022-12-01', 'ratio': 4}, {'period': '2022-12-03', 'ratio': 4}, {'period': '2022-12-05', 'ratio': 4}, {'period': '2022-12-06', 'ratio': 2}, {'period': '2022-12-08', 'ratio': 2}, {'period': '2022-12-14', 'ratio': 2}, {'period': '2022-12-19', 'ratio': 2}, {'period': '2022-12-22', 'ratio': 2}, {'period': '2022-12-23', 'ratio': 2}, {'period': '2023-01-01', 'ratio': 2}]}, {'title': '영어', 'keywords': ['영어', 'english'], 'data': [{'period': '2022-10-01', 'ratio': 28}, {'period': '2022-10-02', 'ratio': 14}, {'period': '2022-10-03', 'ratio': 20}, {'period': '2022-10-04', 'ratio': 18}, {'period': '2022-10-05', 'ratio': 12}, {'period': '2022-10-06', 'ratio': 16}, {'period': '2022-10-07', 'ratio': 10}, {'period': '2022-10-08', 'ratio': 12}, {'period': '2022-10-09', 'ratio': 14}, {'period': '2022-10-10', 'ratio': 20}, {'period': '2022-10-11', 'ratio': 32}, {'period': '2022-10-12', 'ratio': 28}, {'period': '2022-10-13', 'ratio': 16}, {'period': '2022-10-14', 'ratio': 26}, {'period': '2022-10-15', 'ratio': 26}, {'period': '2022-10-16', 'ratio': 6}, {'period': '2022-10-17', 'ratio': 38}, {'period': '2022-10-18', 'ratio': 22}, {'period': '2022-10-19', 'ratio': 24}, {'period': '2022-10-20', 'ratio': 24}, {'period': '2022-10-21', 'ratio': 14}, {'period': '2022-10-22', 'ratio': 16}, {'period': '2022-10-23', 'ratio': 30}, {'period': '2022-10-24', 'ratio': 20}, {'period': '2022-10-25', 'ratio': 30}, {'period': '2022-10-26', 'ratio': 22}, {'period': '2022-10-27', 'ratio': 26}, {'period': '2022-10-28', 'ratio': 6}, {'period': '2022-10-29', 'ratio': 20}, {'period': '2022-10-30', 'ratio': 34}, {'period': '2022-10-31', 'ratio': 28}, {'period': '2022-11-01', 'ratio': 46}, {'period': '2022-11-02', 'ratio': 24}, {'period': '2022-11-03', 'ratio': 36}, {'period': '2022-11-04', 'ratio': 22}, {'period': '2022-11-05', 'ratio': 18}, {'period': '2022-11-06', 'ratio': 32}, {'period': '2022-11-07', 'ratio': 24}, {'period': '2022-11-08', 'ratio': 30}, {'period': '2022-11-09', 'ratio': 30}, {'period': '2022-11-10', 'ratio': 24}, {'period': '2022-11-11', 'ratio': 14}, {'period': '2022-11-12', 'ratio': 20}, {'period': '2022-11-13', 'ratio': 36}, {'period': '2022-11-14', 'ratio': 40}, {'period': '2022-11-15', 'ratio': 32}, {'period': '2022-11-16', 'ratio': 32}, {'period': '2022-11-17', 'ratio': 100}, {'period': '2022-11-18', 'ratio': 44}, {'period': '2022-11-19', 'ratio': 28}, {'period': '2022-11-20', 'ratio': 52}, {'period': '2022-11-21', 'ratio': 32}, {'period': '2022-11-22', 'ratio': 22}, {'period': '2022-11-23', 'ratio': 18}, {'period': '2022-11-24', 'ratio': 34}, {'period': '2022-11-25', 'ratio': 36}, {'period': '2022-11-26', 'ratio': 26}, {'period': '2022-11-27', 'ratio': 26}, {'period': '2022-11-28', 'ratio': 24}, {'period': '2022-11-29', 'ratio': 38}, {'period': '2022-11-30', 'ratio': 12}, {'period': '2022-12-01', 'ratio': 20}, {'period': '2022-12-02', 'ratio': 18}, {'period': '2022-12-03', 'ratio': 12}, {'period': '2022-12-04', 'ratio': 26}, {'period': '2022-12-05', 'ratio': 24}, {'period': '2022-12-06', 'ratio': 16}, {'period': '2022-12-07', 'ratio': 20}, {'period': '2022-12-08', 'ratio': 32}, {'period': '2022-12-09', 'ratio': 20}, {'period': '2022-12-10', 'ratio': 12}, {'period': '2022-12-11', 'ratio': 20}, {'period': '2022-12-12', 'ratio': 24}, {'period': '2022-12-13', 'ratio': 22}, {'period': '2022-12-14', 'ratio': 34}, {'period': '2022-12-15', 'ratio': 20}, {'period': '2022-12-16', 'ratio': 16}, {'period': '2022-12-17', 'ratio': 22}, {'period': '2022-12-18', 'ratio': 50}, {'period': '2022-12-19', 'ratio': 42}, {'period': '2022-12-20', 'ratio': 46}, {'period': '2022-12-21', 'ratio': 42}, {'period': '2022-12-22', 'ratio': 30}, {'period': '2022-12-23', 'ratio': 22}, {'period': '2022-12-24', 'ratio': 10}, {'period': '2022-12-25', 'ratio': 18}, {'period': '2022-12-26', 'ratio': 48}, {'period': '2022-12-27', 'ratio': 30}, {'period': '2022-12-28', 'ratio': 22}, {'period': '2022-12-29', 'ratio': 14}, {'period': '2022-12-30', 'ratio': 16}, {'period': '2022-12-31', 'ratio': 8}, {'period': '2023-01-01', 'ratio': 8}]}]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          date  seraching_result\n",
              "0   2022-10-02                 2\n",
              "1   2022-10-03                 6\n",
              "2   2022-10-04                 2\n",
              "3   2022-10-05                 8\n",
              "4   2022-10-10                 2\n",
              "5   2022-10-11                 4\n",
              "6   2022-10-13                 4\n",
              "7   2022-10-15                 4\n",
              "8   2022-10-16                 2\n",
              "9   2022-10-17                 2\n",
              "10  2022-10-18                 2\n",
              "11  2022-10-21                 2\n",
              "12  2022-10-25                 2\n",
              "13  2022-10-26                 4\n",
              "14  2022-10-28                 2\n",
              "15  2022-10-31                 4\n",
              "16  2022-11-02                 4\n",
              "17  2022-11-05                 2\n",
              "18  2022-11-07                 6\n",
              "19  2022-11-09                 4\n",
              "20  2022-11-11                 2\n",
              "21  2022-11-14                 4\n",
              "22  2022-11-16                 4\n",
              "23  2022-11-17                 2\n",
              "24  2022-11-18                 2\n",
              "25  2022-11-19                 2\n",
              "26  2022-11-20                 2\n",
              "27  2022-11-22                 2\n",
              "28  2022-11-26                 2\n",
              "29  2022-11-27                 2\n",
              "30  2022-11-28                 4\n",
              "31  2022-11-30                 4\n",
              "32  2022-12-01                 4\n",
              "33  2022-12-03                 4\n",
              "34  2022-12-05                 4\n",
              "35  2022-12-06                 2\n",
              "36  2022-12-08                 2\n",
              "37  2022-12-14                 2\n",
              "38  2022-12-19                 2\n",
              "39  2022-12-22                 2\n",
              "40  2022-12-23                 2\n",
              "41  2023-01-01                 2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b38de84-c177-47af-8501-635f00b7b72f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>seraching_result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-10-02</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-10-03</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-10-04</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-10-05</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-10-10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2022-10-11</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2022-10-13</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2022-10-15</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2022-10-16</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2022-10-17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2022-10-18</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2022-10-21</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2022-10-26</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2022-10-28</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2022-10-31</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2022-11-05</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2022-11-07</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2022-11-09</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2022-11-11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2022-11-14</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2022-11-16</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2022-11-17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2022-11-18</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2022-11-19</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2022-11-22</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2022-11-26</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2022-11-27</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2022-11-28</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2022-11-30</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2022-12-01</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>2022-12-03</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2022-12-05</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2022-12-06</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>2022-12-08</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2022-12-14</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2022-12-19</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2022-12-22</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2022-12-23</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b38de84-c177-47af-8501-635f00b7b72f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b38de84-c177-47af-8501-635f00b7b72f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b38de84-c177-47af-8501-635f00b7b72f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n",
        "\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "BlnH1d802Szy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모형 생성\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state = 0, n_estimators = 300, max_depth = 3)\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y_predict value: \\m\", Y_pred)\n",
        "print(\"accuracy(test): {:.3f}\".format(model.score(X_test, Y_test)))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M43iNeUW3HRr",
        "outputId": "b7a3b3d9-f55d-4b6f-f205-84586f6ee483"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_predict value: \\m [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test): 0.947\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.96       177\n",
            "           1       0.99      0.88      0.93       123\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.96      0.94      0.94       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)"
      ],
      "metadata": {
        "id": "vGkzBwFh5EUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모형생성\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor(random_state = 0, n_estimators = 100, max_depth = 4)\n",
        "\n",
        "#5. 모형 에측 평가\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(R2) : {:.3f}\". format(model.score(X_train, Y_train)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE :\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHRh3Qk46uod",
        "outputId": "5c6d695d-7a2c-4e5a-9f12-5581078f4080"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [1.         1.         0.11744215 0.05016315 1.         1.\n",
            " 0.07238019 0.11241476 0.13835006 1.         0.13030363 0.12825997\n",
            " 1.         0.11692024 1.         0.09050479 1.         0.0842891\n",
            " 0.12137086 0.11798424 0.08577163 0.11223331 0.11070843 0.13532671\n",
            " 0.12739543 1.         0.08289355 0.08577163 0.12400498 0.05647365\n",
            " 1.         1.         1.         1.         0.11045272 1.\n",
            " 1.         0.08577163 1.         0.1244963  0.0543375  1.\n",
            " 1.         1.         0.11728393 0.05047439 1.         0.11959046\n",
            " 0.3518854  0.05047439 1.         0.11847929 0.11038671 0.09449241\n",
            " 1.         1.         0.0842891  0.09050479 1.         1.\n",
            " 0.11220638 1.         1.         0.14918503 0.11120341 0.11341773\n",
            " 1.         0.14023568 0.11940419 0.08071439 1.         0.27805003\n",
            " 0.0896234  0.13326402 1.         0.08364049 0.11241476 0.08617314\n",
            " 0.13877628 1.         0.08577163 0.11262796 0.0841049  1.\n",
            " 1.         1.         0.11940419 0.11049757 0.11074738 0.05647365\n",
            " 1.         0.07978873 0.08577163 0.11214618 0.09258329 1.\n",
            " 1.         0.11326787 0.11049757 0.10079204 1.         1.\n",
            " 0.11532762 1.         0.05177874 1.         1.         0.05564185\n",
            " 1.         1.         0.09177481 1.         0.11704153 0.08675666\n",
            " 0.1198634  0.11038671 0.11241476 0.08967331 0.09460367 0.09139421\n",
            " 1.         0.08208844 0.07978873 1.         0.12807789 1.\n",
            " 0.11692024 0.08577163 1.         1.         0.08577163 0.0896234\n",
            " 1.         1.         1.         1.         0.15032492 1.\n",
            " 0.11418409 1.         0.11145569 0.25357764 0.0837648  0.08777536\n",
            " 0.18959652 1.         0.0841049  0.11038671 1.         1.\n",
            " 0.09028105 0.13241758 0.14335325 0.08487147 0.08753109 1.\n",
            " 0.11526838 0.08601845 0.0942563  0.08217476 1.         1.\n",
            " 0.08698298 0.11638698 1.         0.08364049 0.1140496  1.\n",
            " 0.11038671 0.13286468 0.14060319 1.         0.1397672  1.\n",
            " 0.08577163 0.20901477 0.09089904 0.08577163 1.         0.11341773\n",
            " 0.05177874 0.07978873 0.11704153 0.11038671 1.         0.08577163\n",
            " 0.07314258 1.         1.         1.         0.05177874 1.\n",
            " 0.11496946 1.         1.         1.         1.         0.09788847\n",
            " 1.         0.0799326  0.10906861 0.08214297 0.08201874 1.\n",
            " 1.         0.10339469 1.         0.1150795  0.11223331 1.\n",
            " 0.08733028 0.13671359 0.0888653  0.15638295 0.09122832 1.\n",
            " 0.12053685 0.13671359 0.08577163 1.         1.         1.\n",
            " 1.         0.08201874 0.13426699 1.         1.         0.08047499\n",
            " 0.11483733 1.         0.11141661 0.08617314 0.07914185 0.10991048\n",
            " 0.11873367 0.10977558 0.20668476 0.12046224 0.14211478 0.08024315\n",
            " 0.11141661 0.27430871 0.11334046 0.07183824 1.         0.05047439\n",
            " 1.         0.11835696 1.         1.         0.11373061 1.\n",
            " 0.12423424 0.08963474 0.05647365 1.         1.         0.0770057\n",
            " 0.1204868  0.12329508 1.         1.         0.13003007 0.08971736\n",
            " 0.10773749 0.1140496  0.08617314 0.08577163 1.         0.11241476\n",
            " 1.         0.12193411 0.11962094 0.10888877 1.         1.\n",
            " 0.1170146  0.08447685 0.118801   0.08021272 1.         1.\n",
            " 0.13059789 1.         0.09590904 0.08487147 1.         0.1102256\n",
            " 0.16014576 0.08577163 1.         0.12881321 1.         1.\n",
            " 0.09158032 0.17120978 0.12603675 0.08154819 0.0886603  0.09285669]\n",
            "accuracy(R2) : 0.785\n",
            "RMSE : 0.22498174275211433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그리디안 부스트"
      ],
      "metadata": {
        "id": "Vb7LS6zkA67i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[[\"총매출액\", \"구매금액대\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]]\n",
        "Y = df[\"할인민감여부\"]\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"총매출액\", \"1회 평균매출액\", \"평균 구매주기\", \"거래기간\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"구매금액대\"])])\n",
        "ct.fit(X_train)\n",
        "\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)\n",
        "\n",
        "smote = SMOTE(random_state = 0)\n",
        "X_train, Y_train = smote.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "LbmXOWiIAu6A"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모형 생성\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(random_state=0, n_estimators = 100, max_depth = 4, learning_rate = 0.1)\n",
        "\n",
        "#모형 학습 예측\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y_predict value: \\m\", Y_pred)\n",
        "print(\"accuracy(test): {:.3f}\".format(model.score(X_test, Y_test)))\n",
        "print(classification_report(Y_test, Y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFuAOwXRA_Ty",
        "outputId": "954d4e3a-aa05-4a8b-a1d4-12ea627c1788"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_predict value: \\m [1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0]\n",
            "accuracy(test): 0.933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.97      0.94       177\n",
            "           1       0.95      0.89      0.92       123\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.94      0.93      0.93       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#변수선택\n",
        "X = df[df.이탈여부 == 0][[\"방문빈도\", \"총 할인 금액\", \"고객등급\", \"구매유형\", \"거래기간\", \"할인민감여부\", \"평균 구매주기\"]]\n",
        "Y = np.log1p(df[df.이탈여부 == 0][\"1회 평균매출액\"])\n",
        "\n",
        "#데이터 분할(train, test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
        "\n",
        "#데이터 전처리\n",
        "ct = ColumnTransformer([('scaling', StandardScaler(), [\"방문빈도\", \"총 할인 금액\", \"거래기간\", \"평균 구매주기\"]),\n",
        "                        (\"onehot\", OneHotEncoder(sparse = False), [\"고객등급\", \"구매유형\", \"할인민감여부\"])])\n",
        "ct.fit(X_train)\n",
        "X_train= ct.transform(X_train)\n",
        "X_test= ct.transform(X_test)"
      ],
      "metadata": {
        "id": "b3yrBlGPBdqL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모형생성\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(random_state = 0, n_estimators = 100, max_depth =4, learning_rate = 0.1)\n",
        "\n",
        "#5. 모형 에측 평가\n",
        "model.fit(X_train, Y_train)\n",
        "Y_pred = model.predict(X_test)\n",
        "print(\"Y predict value : \\n\", Y_pred)\n",
        "print(\"accuracy(R2) : {:.3f}\". format(model.score(X_train, Y_train)))\n",
        "rmse = sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "print(\"RMSE :\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUsi6i33C0hZ",
        "outputId": "dd3b56ed-17cf-46a1-9b0f-9f7c7c0ffd10"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y predict value : \n",
            " [12.62344813 12.25583406 12.09458751 13.07090617 11.40422303 12.45220348\n",
            " 11.61198904 12.45651119 12.08328814 12.32640447 12.39533751 12.23594472\n",
            " 12.13577357 12.24057416 12.73503493 12.29364736 12.84361351 12.99742565\n",
            " 11.76512764 12.45752565 12.63483127 11.38579233 12.41560355 12.50712961\n",
            " 10.79435548 12.62928969 12.60613938 11.92964399 12.34916462 12.07520652\n",
            " 12.57121312 13.01697524 12.39965923 13.03165827 12.11711146 12.09888134\n",
            " 12.18727902 12.94489209 11.93717047 13.18376863 12.76957258 12.21597158\n",
            " 12.54049819 12.10678757 12.64544285 12.59000213 11.18198188 12.13358256\n",
            " 12.5668075  12.42634623 11.81913361 13.52008205 11.10458182 12.09979475\n",
            " 13.07232151 12.20069308 12.66489797 12.28476526 11.98447614 12.31654311\n",
            " 12.79736561 12.76535462 11.92863698 12.44145179 12.12157567 12.10310068\n",
            " 11.87519738 11.62364672 12.99672955 12.27936785 12.36258046 11.70831635\n",
            " 11.91861812 11.96233767 12.15305515 13.12437922 12.65319979 12.93053239\n",
            " 12.03260309 12.25751434 12.0000299  11.89763423 11.5329378  12.93737778\n",
            " 12.40440044 12.64050116 12.73633832 12.62223461 11.7388871  12.5605956\n",
            " 12.46170754 13.27233418 12.49065444 12.99839492 11.98770272 13.31580316\n",
            " 13.01839043 12.06375631 12.80479374 11.98398262 11.65401981 12.2787555\n",
            " 12.73424086 11.43216014 12.24460123 12.8969792  12.85568613 11.95915872\n",
            " 12.04694305 12.60039447 12.01332333 11.79424302 12.69414998 13.32348351\n",
            " 11.70900777 12.09118735 12.18245925 12.88276381 12.17369179 12.50951424\n",
            " 12.72249627 12.60660354 12.17592955 12.74111678 12.65434081 12.24543644\n",
            " 12.44711341 11.5965925  12.66089249 11.8160437  12.76149045 11.7436314\n",
            " 13.03109978 12.47939971 11.51054282 12.14448715 11.47094125 12.11272934\n",
            " 12.91853515 11.72347747 12.41600997 12.02868338 12.60354515 11.74879952\n",
            " 12.5937395  12.63427503 12.89215104 12.73329149 12.58830131 11.85372773\n",
            " 12.44993728 12.89737544 12.4613579  12.89249471 13.5522812  11.48000074\n",
            " 11.50736548 12.67931101 12.15982715 12.60083252 13.03940439 12.73213993\n",
            " 12.75970949 11.94740293 12.66238056 12.38212172 12.7089022  11.21000233\n",
            " 12.72847666 12.15336244 11.21970406 11.8446597  11.87301358 13.47804771\n",
            " 12.35228515 12.60249735 13.83729506 11.86136594 12.63613548 11.25707904\n",
            " 13.54480221 13.48288458 12.66406609 12.72222242 12.05683865 11.94755448\n",
            " 12.37945452 12.37595316 10.65869553 13.10374396 12.37640418 12.58991922\n",
            " 13.19704136 12.59201926 11.85361937 12.02339775 12.40938986 11.80883328\n",
            " 13.01162095 11.53925824 13.19678706 12.58870571 12.0776148  11.41688057\n",
            " 12.70617545 11.84467509 12.43060262 12.42450198 12.92575457 12.54130517]\n",
            "accuracy(R2) : 0.916\n",
            "RMSE : 0.41380793566137386\n"
          ]
        }
      ]
    }
  ]
}